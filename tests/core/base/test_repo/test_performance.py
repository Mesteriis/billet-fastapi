"""
Performance —Ç–µ—Å—Ç—ã –¥–ª—è —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö.

–ü–æ–∫—Ä—ã–≤–∞–µ—Ç:
- –†–∞–±–æ—Ç—É —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏ (1000+ –∑–∞–ø–∏—Å–µ–π)
- –ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –∞–≥—Ä–µ–≥–∞—Ü–∏–π
- Bulk –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π
- Edge cases –¥–ª—è –∫—É—Ä—Å–æ—Ä–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
- –ü–æ–ª–Ω–æ—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫ –Ω–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–∞—Ö
- –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ
"""

import asyncio
import logging
import random
import time
import uuid
from datetime import date, datetime, timedelta
from typing import Any

import pytest
from pytz import utc
from sqlalchemy import func, select
from sqlalchemy.ext.asyncio import AsyncSession

from core.base.repo.repository import BaseRepository
from tests.utils_test.isolation_decorators import database_reset_test

from .conftest import post_repo, tag_repo, user_repo
from .enums import PostStatus, Priority
from .factories import TestPostFactory, TestTagFactory, TestUserFactory, fake
from .modesl_for_test import TestPost, TestUser

# –ò—Å–ø–æ–ª—å–∑—É–µ–º logger –∏–∑ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ conftest.py
logger = logging.getLogger("test_session")


@pytest.mark.performance
@database_reset_test(verbose=True)
async def test_bulk_create_large_dataset(setup_test_models, user_repo):
    """
    –¢–µ—Å—Ç —Å–æ–∑–¥–∞–Ω–∏—è –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —á–µ—Ä–µ–∑ bulk_create.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å –æ–±—Ä–∞–±–æ—Ç–∫–∏ –±–æ–ª—å—à–∏—Ö –±–∞—Ç—á–µ–π.
    """
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (1000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)")

    start_time = time.time()

    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—Å—Å–æ–≤–æ–≥–æ —Å–æ–∑–¥–∞–Ω–∏—è
    users_data = []
    for i in range(1000):
        users_data.append(
            {
                "username": f"perf_user_{i}_{uuid.uuid4().hex[:8]}",
                "email": f"perf_{i}_{uuid.uuid4().hex[:8]}@example.com",
                "full_name": f"Performance User {i}",
                "hashed_password": "test_password_hash",
                "is_active": random.choice([True, False]),
                "is_verified": random.choice([True, False]),
                "is_superuser": i < 50,  # –ü–µ—Ä–≤—ã–µ 50 - —Å—É–ø–µ—Ä–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏
            }
        )

    # –ú–∞—Å—Å–æ–≤–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å —Ä–∞–∑–Ω—ã–º–∏ —Ä–∞–∑–º–µ—Ä–∞–º–∏ –±–∞—Ç—á–µ–π
    created_users = await user_repo.bulk_create(
        users_data,
        batch_size=100,  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –±–∞—Ç—á–∏—Ä–æ–≤–∞–Ω–∏–µ
        emit_events=False,  # –û—Ç–∫–ª—é—á–∞–µ–º —Å–æ–±—ã—Ç–∏—è –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    )

    creation_time = time.time() - start_time
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(created_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∑–∞ {creation_time:.2f} —Å–µ–∫—É–Ω–¥")
    logger.info(f"üìä –°–∫–æ—Ä–æ—Å—Ç—å: {len(created_users) / creation_time:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")

    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    assert len(created_users) == 1000, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω–æ 1000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    emails = [user.email for user in created_users]
    assert len(set(emails)) == len(emails), "–í—Å–µ email –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏"

    usernames = [user.username for user in created_users]
    assert len(set(usernames)) == len(usernames), "–í—Å–µ username –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É–Ω–∏–∫–∞–ª—å–Ω—ã–º–∏"

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –ø–æ–ª—è–º
    active_count = sum(1 for user in created_users if user.is_active)
    verified_count = sum(1 for user in created_users if user.is_verified)
    superuser_count = sum(1 for user in created_users if user.is_superuser)

    logger.info(f"üìà –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π:")
    logger.info(f"   –ê–∫—Ç–∏–≤–Ω—ã—Ö: {active_count}")
    logger.info(f"   –í–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö: {verified_count}")
    logger.info(f"   –°—É–ø–µ—Ä–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {superuser_count}")

    assert superuser_count == 50, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Ä–æ–≤–Ω–æ 50 —Å—É–ø–µ—Ä–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"

    # –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–æ–ª–∂–µ–Ω —É–∫–ª–∞–¥—ã–≤–∞—Ç—å—Å—è –≤ —Ä–∞–∑—É–º–Ω–æ–µ –≤—Ä–µ–º—è
    assert creation_time < 30, f"–°–æ–∑–¥–∞–Ω–∏–µ 1000 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∑–∞–Ω—è–ª–æ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: {creation_time:.2f}—Å"


@pytest.mark.performance
@database_reset_test(verbose=True)
async def test_complex_filtering_performance(
    setup_test_models: AsyncSession,
    post_repo: BaseRepository,
) -> None:
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–∂–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏."""
    logger = logging.getLogger("test_session")
    logger.info("üèÉ‚Äç‚ôÇÔ∏è –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–ª–æ–∂–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏")

    # –°–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ—Å—Ç–æ–≤
    authors: list[TestUser] = []
    for i in range(5):
        author = TestUser(
            username=f"perf_author_{fake.uuid4()[:8]}_{i}",
            email=fake.unique.email(),
            full_name=fake.name(),
            hashed_password=fake.password(),
            is_active=True,
            is_verified=i % 2 == 0,
        )
        setup_test_models.add(author)
        authors.append(author)

    await setup_test_models.commit()

    # –°–æ–∑–¥–∞–µ–º –±–æ–ª—å—à–æ–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    posts_batch = []
    for i in range(1000):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ñ–∞–±—Ä–∏–∫—É –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        post = TestPost(
            title=f"Performance Test Post {i}",
            slug=f"perf-test-{i}",
            content=fake.text(max_nb_chars=1000),
            excerpt=fake.text(max_nb_chars=200) if i % 3 == 0 else None,
            status=fake.random_element(elements=[PostStatus.DRAFT, PostStatus.PUBLISHED, PostStatus.ARCHIVED]),
            priority=fake.random_element(elements=[Priority.LOW, Priority.MEDIUM, Priority.HIGH]),
            views_count=fake.random_int(min=0, max=10000),
            likes_count=fake.random_int(min=0, max=500),
            rating=fake.pyfloat(left_digits=1, right_digits=1, positive=True, max_value=5.0) if i % 4 == 0 else None,
            published_at=fake.date_time_this_year(tzinfo=utc) if i % 2 == 0 else None,
            scheduled_at=fake.date_this_year() if i % 5 == 0 else None,
            is_featured=fake.boolean(chance_of_getting_true=20),
            is_premium=fake.boolean(chance_of_getting_true=10),
            allow_comments=fake.boolean(chance_of_getting_true=80),
            extra_metadata={"keywords": fake.words(nb=3), "category": f"cat_{i % 10}"},
            author_id=authors[i % len(authors)].id,
        )
        posts_batch.append(post)

        # –î–æ–±–∞–≤–ª—è–µ–º –∏ –∫–æ–º–º–∏—Ç–∏–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if len(posts_batch) >= 100:
            setup_test_models.add_all(posts_batch)
            await setup_test_models.commit()
            posts_batch = []

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø–æ—Å—Ç—ã
    if posts_batch:
        setup_test_models.add_all(posts_batch)
        await setup_test_models.commit()

    logger.info("–°–æ–∑–¥–∞–Ω–æ 1000 –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")


@pytest.mark.performance
@database_reset_test(verbose=True)
async def test_cursor_pagination_performance(setup_test_models, post_repo):
    """
    –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫—É—Ä—Å–æ—Ä–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏ –Ω–∞ –±–æ–ª—å—à–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç edge cases —Å —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –∫—É—Ä—Å–æ—Ä–æ–≤.
    """
    logger.info("üìÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∫—É—Ä—Å–æ—Ä–Ω–æ–π –ø–∞–≥–∏–Ω–∞—Ü–∏–∏")

    # –°–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ—Å—Ç–æ–≤
    authors: list[TestUser] = []
    for i in range(3):
        author = TestUser(
            username=f"cursor_author_{fake.uuid4()[:8]}_{i}",
            email=fake.unique.email(),
            full_name=fake.name(),
            hashed_password=fake.password(),
            is_active=True,
            is_verified=True,
        )
        setup_test_models.add(author)
        authors.append(author)

    await setup_test_models.commit()

    # –°–æ–∑–¥–∞–µ–º –ø–æ—Å—Ç—ã —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫—É—Ä—Å–æ—Ä–æ–≤
    posts_batch = []
    base_date = datetime.now()

    for i in range(1500):
        post = TestPost(
            title=f"Pagination Post {i:04d}",
            slug=f"pagination-post-{i:04d}",
            content=f"Content {i}",
            views_count=i * 10,  # –ú–æ–Ω–æ—Ç–æ–Ω–Ω–æ –≤–æ–∑—Ä–∞—Å—Ç–∞—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            rating=round((i % 50) / 10.0, 1),  # –¶–∏–∫–ª–∏—á–µ—Å–∫–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–∏
            status=PostStatus.PUBLISHED,
            priority=Priority.MEDIUM,
            likes_count=fake.random_int(min=0, max=100),
            is_featured=fake.boolean(chance_of_getting_true=20),
            is_premium=fake.boolean(chance_of_getting_true=10),
            allow_comments=True,
            author_id=authors[i % len(authors)].id,
            published_at=base_date + timedelta(minutes=i),  # –†–∞–∑–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–∞ —Å–æ–∑–¥–∞–Ω–∏—è
        )
        posts_batch.append(post)

        # –î–æ–±–∞–≤–ª—è–µ–º –±–∞—Ç—á–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if len(posts_batch) >= 300:
            setup_test_models.add_all(posts_batch)
            await setup_test_models.commit()
            posts_batch = []

    # –î–æ–±–∞–≤–ª—è–µ–º –æ—Å—Ç–∞–≤—à–∏–µ—Å—è –ø–æ—Å—Ç—ã
    if posts_batch:
        setup_test_models.add_all(posts_batch)
        await setup_test_models.commit()

    logger.info("–°–æ–∑–¥–∞–Ω–æ 1500 –ø–æ—Å—Ç–æ–≤ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏")


@pytest.mark.performance
@database_reset_test(verbose=True)
async def test_bulk_operations_performance(setup_test_models, user_repo):
    """
    –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ bulk –æ–ø–µ—Ä–∞—Ü–∏–π —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π –∏ edge cases.
    """
    logger.info("üì¶ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å bulk –æ–ø–µ—Ä–∞—Ü–∏–π")

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è bulk –æ–ø–µ—Ä–∞—Ü–∏–π
    base_users_data = []
    for i in range(500):
        base_users_data.append(
            {
                "username": f"bulk_user_{i}_{uuid.uuid4().hex[:8]}",
                "email": f"bulk_{i}@example.com",
                "full_name": f"Bulk User {i}",
                "hashed_password": "bulk_password",
                "is_active": True,
                "is_verified": i % 3 == 0,  # –ö–∞–∂–¥—ã–π —Ç—Ä–µ—Ç–∏–π –≤–µ—Ä–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω
            }
        )

    created_users = await user_repo.bulk_create(base_users_data, emit_events=False)
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(created_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è bulk –æ–ø–µ—Ä–∞—Ü–∏–π")

    # –¢–µ—Å—Ç 1: Bulk Update
    logger.info("üîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º bulk update")
    start_time = time.time()

    updated_count = await user_repo.bulk_update(
        filters={"is_verified": False},
        update_data={"is_verified": True, "email_verified_at": datetime.now()},
        emit_events=False,
    )

    bulk_update_time = time.time() - start_time
    logger.info(f"   Bulk update: {bulk_update_time:.3f}—Å, –æ–±–Ω–æ–≤–ª–µ–Ω–æ {updated_count} –∑–∞–ø–∏—Å–µ–π")

    # –¢–µ—Å—Ç 2: Bulk Update —Å —É—Å–ª–æ–≤–Ω—ã–º–∏ —Ñ–∏–ª—å—Ç—Ä–∞–º–∏
    logger.info("üéØ –¢–µ—Å—Ç–∏—Ä—É–µ–º —É—Å–ª–æ–≤–Ω—ã–π bulk update")
    start_time = time.time()

    conditional_update_count = await user_repo.bulk_update(
        filters={"username__icontains": "bulk_user", "is_active": True, "created_at__date": datetime.now().date()},
        update_data={"last_login_at": datetime.now(), "bio": "Updated via bulk operation"},
        emit_events=False,
    )

    conditional_update_time = time.time() - start_time
    logger.info(
        f"   –£—Å–ª–æ–≤–Ω—ã–π bulk update: {conditional_update_time:.3f}—Å, –æ–±–Ω–æ–≤–ª–µ–Ω–æ {conditional_update_count} –∑–∞–ø–∏—Å–µ–π"
    )

    # –¢–µ—Å—Ç 3: Bulk Delete (soft delete)
    logger.info("üóëÔ∏è –¢–µ—Å—Ç–∏—Ä—É–µ–º bulk soft delete")
    start_time = time.time()

    soft_deleted_count = await user_repo.bulk_delete(
        filters={"username__startswith": "bulk_user_1"},  # –£–¥–∞–ª—è–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π 10-19, 100-199
        soft_delete=True,
        emit_events=False,
    )

    bulk_soft_delete_time = time.time() - start_time
    logger.info(f"   Bulk soft delete: {bulk_soft_delete_time:.3f}—Å, —É–¥–∞–ª–µ–Ω–æ {soft_deleted_count} –∑–∞–ø–∏—Å–µ–π")

    # –¢–µ—Å—Ç 4: –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ soft delete —Ä–∞–±–æ—Ç–∞–µ—Ç
    active_users = await user_repo.list(username__startswith="bulk_user_1", include_deleted=False)
    deleted_users = await user_repo.list(username__startswith="bulk_user_1", include_deleted=True)

    logger.info(f"   –ü–æ—Å–ª–µ soft delete: –∞–∫—Ç–∏–≤–Ω—ã—Ö {len(active_users)}, –≤—Å–µ–≥–æ {len(deleted_users)}")

    # –¢–µ—Å—Ç 5: Bulk –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –±–æ–ª—å—à–∏–º–∏ –±–∞—Ç—á–∞–º–∏
    logger.info("üìä –¢–µ—Å—Ç–∏—Ä—É–µ–º bulk –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏")

    large_dataset = []
    for i in range(1000):
        large_dataset.append(
            {
                "username": f"large_bulk_{i}_{uuid.uuid4().hex[:8]}",
                "email": f"large_bulk_{i}@example.com",
                "full_name": f"Large Bulk User {i}",
                "hashed_password": "large_bulk_password",
            }
        )

    start_time = time.time()
    large_created = await user_repo.bulk_create(
        large_dataset,
        batch_size=200,  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞
        emit_events=False,
    )
    large_bulk_create_time = time.time() - start_time

    logger.info(f"   –ë–æ–ª—å—à–æ–π bulk create: {large_bulk_create_time:.3f}—Å, —Å–æ–∑–¥–∞–Ω–æ {len(large_created)} –∑–∞–ø–∏—Å–µ–π")
    logger.info(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {len(large_created) / large_bulk_create_time:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")

    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    assert updated_count > 0, "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å –æ–±–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–ø–∏—Å–∏"
    assert conditional_update_count > 0, "–£—Å–ª–æ–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –∑–∞—Ç—Ä–æ–Ω—É—Ç—å –∑–∞–ø–∏—Å–∏"
    assert soft_deleted_count > 0, "–î–æ–ª–∂–Ω—ã –±—ã—Ç—å –º—è–≥–∫–æ —É–¥–∞–ª–µ–Ω—ã –∑–∞–ø–∏—Å–∏"
    assert len(deleted_users) > len(active_users), "–£–¥–∞–ª–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö"
    assert len(large_created) == 1000, "–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω–æ 1000 –∑–∞–ø–∏—Å–µ–π –≤ –±–æ–ª—å—à–æ–º bulk create"

    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    assert bulk_update_time < 2.0, f"Bulk update —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π: {bulk_update_time:.3f}—Å"
    assert conditional_update_time < 3.0, f"–£—Å–ª–æ–≤–Ω—ã–π bulk update —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π: {conditional_update_time:.3f}—Å"
    assert bulk_soft_delete_time < 1.5, f"Bulk soft delete —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π: {bulk_soft_delete_time:.3f}—Å"
    assert large_bulk_create_time < 10.0, f"–ë–æ–ª—å—à–æ–π bulk create —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π: {large_bulk_create_time:.3f}—Å"


@pytest.mark.performance
@database_reset_test(verbose=True)
async def test_concurrent_operations_performance(setup_test_models, user_repo):
    """
    –¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π.
    –¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –±–µ–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–µ—Å—Å–∏–∏.
    """
    logger.info("üîÑ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ (–±—ã–ª–æ: –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω—ã–µ)")

    async def create_user_batch(batch_id: int, size: int = 50) -> list:
        """–°–æ–∑–¥–∞–µ—Ç –±–∞—Ç—á –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
        users_data = []
        for i in range(size):
            users_data.append(
                {
                    "username": f"sequential_{batch_id}_{i}_{uuid.uuid4().hex[:8]}",
                    "email": f"sequential_{batch_id}_{i}@example.com",
                    "full_name": f"Sequential User {batch_id}-{i}",
                    "hashed_password": "sequential_password",
                }
            )

        return await user_repo.bulk_create(users_data, emit_events=False)

    async def update_user_batch(batch_id: int) -> int:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –±–∞—Ç—á –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
        return await user_repo.bulk_update(
            filters={"username__contains": f"sequential_{batch_id}"},
            update_data={"bio": f"Updated by batch {batch_id}"},
            emit_events=False,
        )

    async def read_user_batch(batch_id: int) -> list:
        """–ß–∏—Ç–∞–µ—Ç –±–∞—Ç—á –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π."""
        return await user_repo.list(username__contains=f"sequential_{batch_id}", limit=100)

    # –¢–µ—Å—Ç 1: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ (–±—ã–ª–æ: –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–Ω–æ–µ)
    logger.info("üìù –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    start_time = time.time()

    created_batches = []
    for i in range(10):
        batch = await create_user_batch(i, 30)
        created_batches.append(batch)

    sequential_create_time = time.time() - start_time
    total_created = sum(len(batch) for batch in created_batches)

    logger.info(f"   –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ: {sequential_create_time:.3f}—Å, —Å–æ–∑–¥–∞–Ω–æ {total_created} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π")
    logger.info(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {total_created / sequential_create_time:.1f} –∑–∞–ø–∏—Å–µ–π/—Å–µ–∫")

    # –¢–µ—Å—Ç 2: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ
    logger.info("üîÑ –°–º–µ—à–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —á—Ç–µ–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è")
    start_time = time.time()

    mixed_results = []
    for i in range(5):
        result1 = await read_user_batch(i)
        result2 = await update_user_batch(i)
        result3 = await read_user_batch(i)
        mixed_results.extend([result1, result2, result3])

    mixed_operations_time = time.time() - start_time

    logger.info(f"   –°–º–µ—à–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏: {mixed_operations_time:.3f}—Å")

    # –¢–µ—Å—Ç 3: –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    logger.info("üìä –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏")
    start_time = time.time()

    counts = []
    counts.append(await user_repo.count(is_active=True))
    counts.append(await user_repo.count(is_verified=True))
    counts.append(await user_repo.count(is_superuser=False))
    counts.append(await user_repo.count(username__contains="sequential"))
    counts.append(await user_repo.count(email__endswith="@example.com"))

    sequential_count_time = time.time() - start_time

    logger.info(f"   –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥—Å—á–µ—Ç—ã: {sequential_count_time:.3f}—Å")
    for i, count in enumerate(counts):
        logger.info(f"     –ü–æ–¥—Å—á–µ—Ç {i + 1}: {count} –∑–∞–ø–∏—Å–µ–π")

    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç–∏
    assert total_created == 300, f"–î–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–æ–∑–¥–∞–Ω–æ 300 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —Å–æ–∑–¥–∞–Ω–æ {total_created}"
    assert all(isinstance(batch, list) for batch in created_batches), "–í—Å–µ –±–∞—Ç—á–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å–ø–∏—Å–∫–∞–º–∏"
    assert all(count >= 0 for count in counts), "–í—Å–µ –ø–æ–¥—Å—á–µ—Ç—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏"

    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ (—Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –æ–ø–µ—Ä–∞—Ü–∏–π)
    assert sequential_create_time < 25.0, f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–æ–µ: {sequential_create_time:.3f}—Å"
    assert mixed_operations_time < 15.0, f"–°–º–µ—à–∞–Ω–Ω—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ: {mixed_operations_time:.3f}—Å"
    assert sequential_count_time < 8.0, f"–ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–¥—Å—á–µ—Ç—ã —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–µ: {sequential_count_time:.3f}—Å"


@pytest.mark.performance
@database_reset_test(verbose=True)
async def test_edge_cases_with_large_data(setup_test_models, post_repo, user_repo):
    """
    –¢–µ—Å—Ç edge cases –Ω–∞ –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–∞—Ö.
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≥—Ä–∞–Ω–∏—á–Ω—ã–µ —Å–ª—É—á–∞–∏ –∏ –æ–±—Ä–∞–±–æ—Ç–∫—É –æ—à–∏–±–æ–∫.
    """
    logger.info("‚ö†Ô∏è –¢–µ—Å—Ç–∏—Ä—É–µ–º edge cases —Å –±–æ–ª—å—à–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏")

    # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
    users_data = [
        {
            "username": f"edge_user_{i}",
            "email": f"edge_{i}@example.com",
            "full_name": f"Edge User {i}",
            "hashed_password": "edge_password",
        }
        for i in range(100)
    ]

    created_users = await user_repo.bulk_create(users_data, emit_events=False)
    logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(created_users)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è edge case —Ç–µ—Å—Ç–æ–≤")

    # Edge Case 1: –ü—É—Å—Ç—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã
    logger.info("1Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã")
    empty_filter_result = await user_repo.list()
    assert len(empty_filter_result) > 0, "–ü—É—Å—Ç–æ–π —Ñ–∏–ª—å—Ç—Ä –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"

    # Edge Case 2: –ù–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ö
    logger.info("2Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è")
    nonexistent_result = await user_repo.list(username="nonexistent_user_12345")
    assert len(nonexistent_result) == 0, "–ù–µ—Å—É—â–µ—Å—Ç–≤—É—é—â–∏–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–µ –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω"

    # Edge Case 3: –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏
    logger.info("3Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø–∞–≥–∏–Ω–∞—Ü–∏–∏")

    # –û—á–µ–Ω—å –±–æ–ª—å—à–æ–π –ª–∏–º–∏—Ç
    large_limit_result = await user_repo.list(limit=10000)
    assert len(large_limit_result) <= 10000, "–†–µ–∑—É–ª—å—Ç–∞—Ç –Ω–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å –ª–∏–º–∏—Ç"

    # –ù—É–ª–µ–≤–æ–π –ª–∏–º–∏—Ç
    zero_limit_result = await user_repo.list(limit=0)
    assert len(zero_limit_result) == 0, "–ù—É–ª–µ–≤–æ–π –ª–∏–º–∏—Ç –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç"

    # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è offset/limit (–¥–æ–ª–∂–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ)
    try:
        negative_result = await user_repo.list(offset=-1, limit=-1)
        logger.info("   –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
    except Exception as e:
        logger.info(f"   –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤—ã–∑–≤–∞–ª–∏ –æ—à–∏–±–∫—É (–æ–∂–∏–¥–∞–µ–º–æ): {e}")

    # Edge Case 4: –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ö
    logger.info("4Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏")

    try:
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É —Å—Ç—Ä–æ–∫–∏ –¥–æ —Ä–∞–∑—É–º–Ω—ã—Ö –ø—Ä–µ–¥–µ–ª–æ–≤
        long_string = "x" * 100  # –í–º–µ—Å—Ç–æ 1000
        long_string_result = await user_repo.list(username__contains=long_string)
        assert len(long_string_result) == 0, "–û—á–µ–Ω—å –¥–ª–∏–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
        logger.info(f"   –î–ª–∏–Ω–Ω–∞—è —Å—Ç—Ä–æ–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–∞: {len(long_string_result)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    except Exception as e:
        logger.info(f"   –û—à–∏–±–∫–∞ —Å –¥–ª–∏–Ω–Ω–æ–π —Å—Ç—Ä–æ–∫–æ–π (–æ–∂–∏–¥–∞–µ–º–æ): {e}")
        # –ü—Ä–∏ –æ—à–∏–±–∫–µ –¥–µ–ª–∞–µ–º rollback —Å–µ—Å—Å–∏–∏
        try:
            await user_repo._db.rollback()
        except:
            pass

    # Edge Case 5: –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ö
    logger.info("5Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã")
    special_chars = ["'", '"', "\\", "%", "_", "NULL", "DROP TABLE", "<script>"]

    for char in special_chars:
        try:
            special_result = await user_repo.list(username__contains=char)
            logger.info(f"   –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–∏–º–≤–æ–ª '{char}' –æ–±—Ä–∞–±–æ—Ç–∞–Ω –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ: {len(special_result)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        except Exception as e:
            logger.warning(f"   –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–∏–º–≤–æ–ª '{char}' –≤—ã–∑–≤–∞–ª –æ—à–∏–±–∫—É: {e}")

    # Edge Case 6: –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ —Ñ–∏–ª—å—Ç—Ä—ã —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –∫–ª—é—á–∞–º–∏
    logger.info("6Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã")
    try:
        conflicting_result = await user_repo.list(
            is_active=True,
            is_active__ne=False,  # –ò–∑–±—ã—Ç–æ—á–Ω–æ–µ —É—Å–ª–æ–≤–∏–µ
        )
        logger.info(f"   –ö–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã: {len(conflicting_result)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
    except Exception as e:
        logger.info(f"   –ö–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ —Ñ–∏–ª—å—Ç—Ä—ã –≤—ã–∑–≤–∞–ª–∏ –æ—à–∏–±–∫—É: {e}")

    # Edge Case 7: Extreme date ranges
    logger.info("7Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –¥–∏–∞–ø–∞–∑–æ–Ω—ã –¥–∞—Ç")

    future_date = datetime(2099, 12, 31)
    past_date = datetime(1900, 1, 1)

    future_result = await user_repo.list(created_at__gte=future_date)
    past_result = await user_repo.list(created_at__lte=past_date)

    assert len(future_result) == 0, "–ë—É–¥—É—â–∏–µ –¥–∞—Ç—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
    assert len(past_result) == 0, "–°–ª–∏—à–∫–æ–º —Å—Ç–∞—Ä—ã–µ –¥–∞—Ç—ã –Ω–µ –¥–æ–ª–∂–Ω—ã –Ω–∞–π—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"

    # Edge Case 8: –ú–∞—Å—Å–æ–≤—ã–µ –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –ø—É—Å—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    logger.info("8Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏–∏ —Å –ø—É—Å—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏")

    empty_bulk_create = await user_repo.bulk_create([], emit_events=False)
    assert len(empty_bulk_create) == 0, "–ü—É—Å—Ç–æ–π bulk create –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫"

    empty_bulk_update = await user_repo.bulk_update(
        filters={"username": "nonexistent"}, update_data={"bio": "test"}, emit_events=False
    )
    assert empty_bulk_update == 0, "Bulk update –±–µ–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–π –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å 0"

    # Edge Case 9: –ì–ª—É–±–æ–∫–æ –≤–ª–æ–∂–µ–Ω–Ω—ã–µ JSON —Ñ–∏–ª—å—Ç—Ä—ã
    logger.info("9Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–ª–æ–∂–Ω—ã–µ JSON –æ–ø–µ—Ä–∞—Ü–∏–∏")

    # –°–æ–∑–¥–∞–µ–º –∞–≤—Ç–æ—Ä–æ–≤ –¥–ª—è –ø–æ—Å—Ç–æ–≤ —Å JSON –¥–∞–Ω–Ω—ã–º–∏
    json_authors = []
    for i in range(3):
        author_data = {
            "username": f"json_author_{i}_{uuid.uuid4().hex[:8]}",
            "email": f"json_author_{i}@example.com",
            "full_name": f"JSON Author {i}",
            "hashed_password": "json_password",
        }
        json_author = await user_repo.create(author_data, emit_event=False)
        json_authors.append(json_author)

    posts_with_json = []
    for i in range(50):
        posts_with_json.append(
            {
                "title": f"JSON Test Post {i}",
                "slug": f"json-test-{i}",
                "content": "JSON test content",
                "author_id": json_authors[i % len(json_authors)].id,  # –î–æ–±–∞–≤–ª—è–µ–º author_id
                "status": PostStatus.DRAFT,
                "priority": Priority.MEDIUM,
                "views_count": 0,
                "likes_count": 0,
                "is_featured": False,
                "is_premium": False,
                "allow_comments": True,
                "extra_metadata": {
                    "level1": {"level2": {"level3": {"value": i, "category": f"cat_{i % 5}"}}},
                    "tags": [f"tag_{j}" for j in range(i % 3 + 1)],
                    "metrics": {"score": i * 2, "views": i * 10},
                },
            }
        )

    created_posts = await post_repo.bulk_create(posts_with_json, emit_events=False)
    logger.info(f"   –°–æ–∑–¥–∞–Ω–æ {len(created_posts)} –ø–æ—Å—Ç–æ–≤ —Å JSON –¥–∞–Ω–Ω—ã–º–∏")

    # –¢–µ—Å—Ç–∏—Ä—É–µ–º JSON —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é
    json_has_key_result = await post_repo.list(extra_metadata__json_has_key="tags")
    logger.info(f"   JSON has_key —Ñ–∏–ª—å—Ç—Ä: {len(json_has_key_result)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    json_has_keys_result = await post_repo.list(extra_metadata__json_has_keys=["level1", "tags"])
    logger.info(f"   JSON has_keys —Ñ–∏–ª—å—Ç—Ä: {len(json_has_keys_result)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

    # Edge Case 10: –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–∏–º–∏—Ç—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    logger.info("üîü –¢–µ—Å—Ç–∏—Ä—É–µ–º –ª–∏–º–∏—Ç—ã –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")

    try:
        # –ü–æ–ª—É—á–∞–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ ID –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ IN –∑–∞–ø—Ä–æ—Å–∞
        existing_users = await user_repo.list(limit=100)
        if existing_users:
            user_ids = [user.id for user in existing_users[: min(50, len(existing_users))]]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–æ 50 ID
            large_in_result = await user_repo.list(id__in=user_ids)
            logger.info(f"   IN –∑–∞–ø—Ä–æ—Å —Å {len(user_ids)} UUID: {len(large_in_result)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")
        else:
            logger.info("   –ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è IN –∑–∞–ø—Ä–æ—Å–∞")
    except Exception as e:
        logger.info(f"   –û—à–∏–±–∫–∞ –≤ –±–æ–ª—å—à–æ–º IN –∑–∞–ø—Ä–æ—Å–µ: {e}")

    logger.info("‚úÖ –í—Å–µ edge case —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
