"""
TestStaticSchema cache service for Enterprise level.

This file was auto-generated by Autogen CLI.
Application: test_static_schemas
Level: Enterprise
Generated at: 2025-06-22T18:01:28.021260
Template Version: v2.0.0
"""

import json
import pickle
from datetime import datetime, timedelta
from typing import Any, Optional, Union, TypeVar, Generic
from dataclasses import dataclass
import asyncio
import hashlib
import logging

import redis.asyncio as redis
from redis.asyncio import Redis

from apps.test_static_schemas.schemas.test_static_schemas_schemas import TestStaticSchemaResponse, TestStaticSchemaList

logger = logging.getLogger("cache.test_static_schemas")

T = TypeVar('T')


@dataclass
class CacheConfig:
    """Configuration for cache service."""
    redis_url: str = "redis://localhost:6379/0"
    default_ttl: int = 3600  # 1 hour
    key_prefix: str = "test_static_schemas"
    compression_enabled: bool = True
    serialization: str = "json"  # json, pickle
    max_retries: int = 3
    retry_delay: float = 0.1


class CacheKey:
    """Cache key builder for TestStaticSchema operations."""
    
    def __init__(self, prefix: str = "test_static_schemas"):
        self.prefix = prefix
    
    def entity(self, entity_id: int) -> str:
        """Generate cache key for single entity."""
        return f"{self.prefix}:teststaticschema:{entity_id}"
    
    def list(self, filters: dict = None, page: int = 1, size: int = 20) -> str:
        """Generate cache key for entity list."""
        # Create deterministic hash from filters
        filter_hash = self._hash_dict(filters or {})
        return f"{self.prefix}:teststaticschema:list:{filter_hash}:p{page}:s{size}"
    
    def search(self, query: str, filters: dict = None) -> str:
        """Generate cache key for search results."""
        filter_hash = self._hash_dict(filters or {})
        query_hash = hashlib.md5(query.encode()).hexdigest()[:8]
        return f"{self.prefix}:teststaticschema:search:{query_hash}:{filter_hash}"
    
    def stats(self, stat_type: str = "general") -> str:
        """Generate cache key for statistics."""
        return f"{self.prefix}:teststaticschema:stats:{stat_type}"
    
    def user_data(self, user_id: int, data_type: str) -> str:
        """Generate cache key for user-specific data."""
        return f"{self.prefix}:user:{user_id}:teststaticschema:{data_type}"
    
    def pattern(self, pattern: str) -> str:
        """Generate cache key pattern for bulk operations."""
        return f"{self.prefix}:teststaticschema:{pattern}"
    
    def _hash_dict(self, data: dict) -> str:
        """Create deterministic hash from dictionary."""
        sorted_items = sorted(data.items())
        content = json.dumps(sorted_items, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()[:8]


class TestStaticSchemaCacheService(Generic[T]):
    """
    Enterprise-grade cache service for TestStaticSchema operations.
    
    Features:
    - Redis-based distributed caching
    - Automatic serialization (JSON/Pickle)
    - TTL management and cache invalidation
    - Compression for large objects
    - Cache warming and preloading
    - Metrics and monitoring
    - Retry logic with exponential backoff
    - Tag-based cache invalidation
    - Multi-level cache support
    
    Example:
        >>> cache_service = TestStaticSchemaCacheService(config)
        >>> 
        >>> # Cache single entity
        >>> await cache_service.set_entity(123, entity_data, ttl=3600)
        >>> entity = await cache_service.get_entity(123)
        >>> 
        >>> # Cache search results
        >>> await cache_service.set_search_results(query, results, ttl=1800)
        >>> results = await cache_service.get_search_results(query)
        >>> 
        >>> # Bulk operations
        >>> await cache_service.invalidate_pattern("teststaticschema:*")
        >>> 
        >>> # Cache warming
        >>> await cache_service.warm_cache(popular_ids)
    """
    
    def __init__(self, config: CacheConfig):
        """Initialize cache service with configuration."""
        self.config = config
        self.redis_client: Optional[Redis] = None
        self.key_builder = CacheKey(config.key_prefix)
        self._connection_pool = None
        
        # Metrics tracking
        self.metrics = {
            "hits": 0,
            "misses": 0,
            "sets": 0,
            "deletes": 0,
            "errors": 0
        }
    
    async def connect(self) -> None:
        """Establish Redis connection with retry logic."""
        for attempt in range(self.config.max_retries):
            try:
                self.redis_client = redis.from_url(
                    self.config.redis_url,
                    decode_responses=False,  # Handle binary data
                    retry_on_timeout=True,
                    socket_keepalive=True,
                    socket_keepalive_options={}
                )
                
                # Test connection
                await self.redis_client.ping()
                logger.info("Connected to Redis successfully")
                return
                
            except Exception as e:
                logger.warning(f"Redis connection attempt {attempt + 1} failed: {e}")
                if attempt < self.config.max_retries - 1:
                    await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                else:
                    logger.error("Failed to connect to Redis after all retries")
                    raise
    
    async def disconnect(self) -> None:
        """Close Redis connection."""
        if self.redis_client:
            await self.redis_client.close()
            logger.info("Disconnected from Redis")
    
    # Entity operations
    
    async def get_entity(self, entity_id: int) -> Optional[TestStaticSchemaResponse]:
        """Get cached entity by ID."""
        try:
            key = self.key_builder.entity(entity_id)
            data = await self._get(key)
            
            if data:
                self.metrics["hits"] += 1
                if isinstance(data, dict):
                    return TestStaticSchemaResponse.model_validate(data)
                return data
            
            self.metrics["misses"] += 1
            return None
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache get entity error: {e}")
            return None
    
    async def set_entity(
        self, 
        entity_id: int, 
        entity: TestStaticSchemaResponse, 
        ttl: Optional[int] = None
    ) -> bool:
        """Cache entity with optional TTL."""
        try:
            key = self.key_builder.entity(entity_id)
            
            # Convert to dict for serialization
            data = entity.model_dump() if hasattr(entity, 'model_dump') else entity
            
            success = await self._set(key, data, ttl or self.config.default_ttl)
            if success:
                self.metrics["sets"] += 1
                
                # Set tags for invalidation
                await self._add_tags(key, [
                    f"entity:{entity_id}",
                    f"status:{data.get('status', 'unknown')}",
                    f"priority:{data.get('priority', 0)}"
                ])
            
            return success
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache set entity error: {e}")
            return False
    
    async def delete_entity(self, entity_id: int) -> bool:
        """Remove entity from cache."""
        try:
            key = self.key_builder.entity(entity_id)
            success = await self._delete(key)
            if success:
                self.metrics["deletes"] += 1
            return success
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache delete entity error: {e}")
            return False
    
    # List operations
    
    async def get_list(
        self, 
        filters: dict = None, 
        page: int = 1, 
        size: int = 20
    ) -> Optional[TestStaticSchemaList]:
        """Get cached list results."""
        try:
            key = self.key_builder.list(filters, page, size)
            data = await self._get(key)
            
            if data:
                self.metrics["hits"] += 1
                return TestStaticSchemaList.model_validate(data)
            
            self.metrics["misses"] += 1
            return None
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache get list error: {e}")
            return None
    
    async def set_list(
        self,
        filters: dict,
        page: int,
        size: int,
        results: TestStaticSchemaList,
        ttl: Optional[int] = None
    ) -> bool:
        """Cache list results."""
        try:
            key = self.key_builder.list(filters, page, size)
            data = results.model_dump()
            
            success = await self._set(key, data, ttl or self.config.default_ttl // 2)
            if success:
                self.metrics["sets"] += 1
                
                # Add tags based on filters
                tags = ["list"]
                if filters:
                    for filter_key, filter_value in filters.items():
                        tags.append(f"filter:{filter_key}:{filter_value}")
                
                await self._add_tags(key, tags)
            
            return success
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache set list error: {e}")
            return False
    
    # Search operations
    
    async def get_search_results(
        self, 
        query: str, 
        filters: dict = None
    ) -> Optional[TestStaticSchemaList]:
        """Get cached search results."""
        try:
            key = self.key_builder.search(query, filters)
            data = await self._get(key)
            
            if data:
                self.metrics["hits"] += 1
                return TestStaticSchemaList.model_validate(data)
            
            self.metrics["misses"] += 1
            return None
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache get search error: {e}")
            return None
    
    async def set_search_results(
        self,
        query: str,
        filters: dict,
        results: TestStaticSchemaList,
        ttl: Optional[int] = None
    ) -> bool:
        """Cache search results."""
        try:
            key = self.key_builder.search(query, filters)
            data = results.model_dump()
            
            success = await self._set(key, data, ttl or self.config.default_ttl // 4)
            if success:
                self.metrics["sets"] += 1
                await self._add_tags(key, ["search", f"query:{hashlib.md5(query.encode()).hexdigest()[:8]}"])
            
            return success
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache set search error: {e}")
            return False
    
    # Statistics operations
    
    async def get_stats(self, stat_type: str = "general") -> Optional[dict]:
        """Get cached statistics."""
        try:
            key = self.key_builder.stats(stat_type)
            data = await self._get(key)
            
            if data:
                self.metrics["hits"] += 1
                return data
            
            self.metrics["misses"] += 1
            return None
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache get stats error: {e}")
            return None
    
    async def set_stats(
        self, 
        stat_type: str, 
        stats_data: dict, 
        ttl: Optional[int] = None
    ) -> bool:
        """Cache statistics data."""
        try:
            key = self.key_builder.stats(stat_type)
            
            success = await self._set(key, stats_data, ttl or 300)  # 5 minutes default
            if success:
                self.metrics["sets"] += 1
                await self._add_tags(key, ["stats", f"type:{stat_type}"])
            
            return success
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache set stats error: {e}")
            return False
    
    # Bulk operations
    
    async def invalidate_pattern(self, pattern: str) -> int:
        """Invalidate cache keys matching pattern."""
        try:
            full_pattern = self.key_builder.pattern(pattern)
            keys = await self.redis_client.keys(full_pattern)
            
            if keys:
                deleted = await self.redis_client.delete(*keys)
                self.metrics["deletes"] += deleted
                logger.info(f"Invalidated {deleted} cache keys matching pattern: {pattern}")
                return deleted
            
            return 0
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache invalidate pattern error: {e}")
            return 0
    
    async def invalidate_by_tags(self, tags: list[str]) -> int:
        """Invalidate cache entries by tags."""
        try:
            deleted = 0
            for tag in tags:
                tag_key = f"{self.config.key_prefix}:tags:{tag}"
                keys = await self.redis_client.smembers(tag_key)
                
                if keys:
                    # Delete tagged keys
                    deleted += await self.redis_client.delete(*keys)
                    # Delete tag set
                    await self.redis_client.delete(tag_key)
                    
            self.metrics["deletes"] += deleted
            logger.info(f"Invalidated {deleted} cache keys by tags: {tags}")
            return deleted
            
        except Exception as e:
            self.metrics["errors"] += 1
            logger.error(f"Cache invalidate by tags error: {e}")
            return 0
    
    async def warm_cache(self, entity_ids: list[int], fetch_callback) -> int:
        """Warm cache with popular entities."""
        try:
            warmed = 0
            for entity_id in entity_ids:
                key = self.key_builder.entity(entity_id)
                
                # Check if already cached
                if not await self.redis_client.exists(key):
                    # Fetch and cache
                    entity = await fetch_callback(entity_id)
                    if entity:
                        await self.set_entity(entity_id, entity)
                        warmed += 1
            
            logger.info(f"Cache warmed with {warmed} entities")
            return warmed
            
        except Exception as e:
            logger.error(f"Cache warming error: {e}")
            return 0
    
    # Cache analysis and monitoring
    
    async def get_cache_info(self) -> dict:
        """Get cache usage information."""
        try:
            info = await self.redis_client.info('memory')
            
            # Get key counts by pattern
            patterns = {
                "entities": f"{self.config.key_prefix}:teststaticschema:*",
                "lists": f"{self.config.key_prefix}:teststaticschema:list:*",
                "searches": f"{self.config.key_prefix}:teststaticschema:search:*",
                "stats": f"{self.config.key_prefix}:teststaticschema:stats:*"
            }
            
            key_counts = {}
            for name, pattern in patterns.items():
                keys = await self.redis_client.keys(pattern)
                key_counts[name] = len(keys)
            
            return {
                "memory_usage": info.get('used_memory_human', 'unknown'),
                "key_counts": key_counts,
                "metrics": self.metrics.copy(),
                "hit_rate": self._calculate_hit_rate()
            }
            
        except Exception as e:
            logger.error(f"Get cache info error: {e}")
            return {"error": str(e)}
    
    def _calculate_hit_rate(self) -> float:
        """Calculate cache hit rate."""
        total_requests = self.metrics["hits"] + self.metrics["misses"]
        if total_requests == 0:
            return 0.0
        return (self.metrics["hits"] / total_requests) * 100
    
    # Private helper methods
    
    async def _get(self, key: str) -> Any:
        """Internal get with serialization handling."""
        if not self.redis_client:
            return None
            
        data = await self.redis_client.get(key)
        if data is None:
            return None
        
        return self._deserialize(data)
    
    async def _set(self, key: str, value: Any, ttl: int) -> bool:
        """Internal set with serialization handling."""
        if not self.redis_client:
            return False
        
        serialized_data = self._serialize(value)
        
        if ttl > 0:
            return await self.redis_client.setex(key, ttl, serialized_data)
        else:
            return await self.redis_client.set(key, serialized_data)
    
    async def _delete(self, key: str) -> bool:
        """Internal delete operation."""
        if not self.redis_client:
            return False
        
        return bool(await self.redis_client.delete(key))
    
    async def _add_tags(self, key: str, tags: list[str]) -> None:
        """Add tags to key for tag-based invalidation."""
        try:
            for tag in tags:
                tag_key = f"{self.config.key_prefix}:tags:{tag}"
                await self.redis_client.sadd(tag_key, key)
                # Set TTL on tag set
                await self.redis_client.expire(tag_key, self.config.default_ttl * 2)
        except Exception as e:
            logger.warning(f"Error adding tags: {e}")
    
    def _serialize(self, data: Any) -> bytes:
        """Serialize data based on configuration."""
        if self.config.serialization == "pickle":
            serialized = pickle.dumps(data)
        else:  # json
            serialized = json.dumps(data, default=str).encode()
        
        if self.config.compression_enabled and len(serialized) > 1024:
            import gzip
            serialized = gzip.compress(serialized)
        
        return serialized
    
    def _deserialize(self, data: bytes) -> Any:
        """Deserialize data based on configuration."""
        # Check if compressed
        if self.config.compression_enabled:
            try:
                import gzip
                data = gzip.decompress(data)
            except:
                pass  # Not compressed
        
        if self.config.serialization == "pickle":
            return pickle.loads(data)
        else:  # json
            return json.loads(data.decode())


# Convenience functions for service integration

async def get_cache_service(config: CacheConfig = None) -> TestStaticSchemaCacheService:
    """Get configured cache service instance."""
    if config is None:
        config = CacheConfig()
    
    cache_service = TestStaticSchemaCacheService(config)
    await cache_service.connect()
    return cache_service 